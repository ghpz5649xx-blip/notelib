# apps/executions/services.py
"""
Service d'orchestration des exÃ©cutions de pipelines.

Modes supportÃ©s :
1. Synchrone (dev) : exÃ©cution sÃ©quentielle bloquante
2. Asynchrone (prod) : dÃ©lÃ©gation Ã  Celery workers

Ce fichier implÃ©mente l'orchestration synchrone.
L'orchestration asynchrone est dans tasks.py (Celery).
"""
import logging
from typing import Dict, Any, List, Optional
from django.db import transaction
from django.utils import timezone

from .models import PipelineRun, StepRun, ExecutionLog
from .sandbox import feature_sandbox, SandboxExecutionError
from pipelines.models import Pipeline
from pipelines.services import pipeline_service
from artefacts.services import artefact_service
from features.models import FeatureMeta
import json

logger = logging.getLogger("notelib")


class ExecutionService:
    """
    Service d'orchestration des exÃ©cutions de pipeline.
    
    ImplÃ©mentation synchrone :
    - Parcours topologique du DAG
    - ExÃ©cution sÃ©quentielle des steps
    - Propagation des artefacts entre steps
    """
    
    def __init__(self):
        self.sandbox = feature_sandbox
    
    def create_run(
        self,
        pipeline: Pipeline,
        input_manifest: Dict[str, Any],
        initiator,
        execution_mode: str = 'sync'
    ) -> PipelineRun:
        """
        CrÃ©e un PipelineRun et ses StepRuns.
        
        Args:
            pipeline: Pipeline Ã  exÃ©cuter
            input_manifest: Inputs fournis
            initiator: Utilisateur
            execution_mode: 'sync' ou 'async'
        
        Returns:
            Instance PipelineRun
        
        Raises:
            ValueError: Si pipeline invalide
        """
        # Validation du pipeline
        if not pipeline.is_valid:
            raise ValueError(
                f"Pipeline is invalid: {pipeline.validation_errors}"
            )
        
        if not pipeline.is_active:
            raise ValueError("Pipeline is not active")
        
        with transaction.atomic():
            # CrÃ©ation du run
            run = PipelineRun.objects.create(
                pipeline=pipeline,
                initiator=initiator,
                input_manifest=input_manifest,
                execution_mode=execution_mode,
                status='PENDING',
            )
            
            # CrÃ©ation des StepRuns pour chaque node
            nodes = pipeline.get_nodes()
            for node in nodes:
                node_id = node['id']
                feature_name = node.get('feature_name')
                feature_hash = node.get('feature_hash')
                
                # RÃ©cupÃ©ration de la feature
                if feature_hash:
                    feature = FeatureMeta.objects.filter(hash=feature_hash).first()
                elif feature_name:
                    feature = FeatureMeta.objects.filter(name=feature_name).first()
                else:
                    logger.warning(f"Node {node_id} has no feature reference")
                    continue
                
                if not feature:
                    logger.warning(
                        f"Feature not found for node {node_id}: "
                        f"name={feature_name}, hash={feature_hash}"
                    )
                    continue
                
                StepRun.objects.create(
                    pipeline_run=run,
                    node_id=node_id,
                    feature_name=feature.name,
                    feature_hash=feature.hash,
                    status='PENDING',
                )
            
            logger.info(
                f"âœ… PipelineRun created: {run.id} "
                f"({len(nodes)} steps, mode={execution_mode})"
            )
        
        return run
    
    def execute_sync(self, run_id: str) -> PipelineRun:
        """
        ExÃ©cute un pipeline en mode synchrone (bloquant).
        
        Workflow :
        1. Calcul du tri topologique
        2. Pour chaque step dans l'ordre :
           - VÃ©rifie dÃ©pendances satisfaites
           - ExÃ©cute dans sandbox
           - CrÃ©e artefact
           - Propage aux steps suivants
        
        Args:
            run_id: UUID du PipelineRun
        
        Returns:
            PipelineRun mis Ã  jour
        
        Raises:
            Exception: Si exÃ©cution Ã©choue
        """
        run = PipelineRun.objects.select_related('pipeline').get(id=run_id)
        
        try:
            # Marque le run comme dÃ©marrÃ©
            run.mark_running()
            
            # Calcul de l'ordre d'exÃ©cution
            execution_order = pipeline_service.topological_sort(run.pipeline.graph)
            
            logger.info(
                f"ðŸš€ Starting sync execution: {run.id}\n"
                f"Order: {' -> '.join(execution_order)}"
            )
            
            # ExÃ©cution sÃ©quentielle
            for node_id in execution_order:
                step_run = StepRun.objects.get(
                    pipeline_run=run,
                    node_id=node_id
                )
                
                try:
                    self._execute_step(run, step_run)
                except Exception as e:
                    logger.error(f"Step {node_id} failed: {e}", exc_info=True)
            
            # VÃ©rification du statut global
            failed_steps = run.step_runs.filter(status='FAILED').count()
            success_steps = run.step_runs.filter(status='SUCCESS').count()
            
            if failed_steps > 0:
                run.mark_failed(
                    f"{failed_steps} step(s) failed, {success_steps} succeeded"
                )
            else:
                run.mark_success()
                logger.info(f"âœ… Pipeline execution completed: {run.id}")
            
            return run
        
        except Exception as e:
            logger.error(f"Pipeline execution failed: {e}", exc_info=True)
            run.mark_failed(str(e))
            raise
    
    def _execute_step(self, run: PipelineRun, step: StepRun):
        """
        ExÃ©cute un StepRun individuel.
        
        Workflow :
        1. RÃ©sout les inputs depuis les artefacts des Ã©tapes prÃ©cÃ©dentes
        2. ExÃ©cute la feature dans sandbox
        3. CrÃ©e l'artefact de sortie
        4. Met Ã  jour le StepRun
        
        Args:
            run: PipelineRun parent
            step: StepRun Ã  exÃ©cuter
        
        Raises:
            Exception: Si exÃ©cution Ã©choue
        """
        logger.info(f"â–¶ï¸  Executing step: {step.node_id} ({step.feature_name})")
        
        step.mark_running()
        
        try:
            # 1. RÃ©solution des inputs
            inputs = self._resolve_inputs(run, step)
            
            # 2. ExÃ©cution dans sandbox
            result_bytes, metadata = self.sandbox.execute_feature(
                feature_hash=step.feature_hash,
                inputs=inputs
            )
            
            # 3. DÃ©sÃ©rialisation du rÃ©sultat
            import cloudpickle
            result_obj = cloudpickle.loads(result_bytes)
            
            # 4. CrÃ©ation de l'artefact
            artefact = artefact_service.create_artefact(
                obj=result_obj,
                feature_hash=step.feature_hash,
                meta={
                    'pipeline_run_id': str(run.id),
                    'step_run_id': str(step.id),
                    'node_id': step.node_id,
                    'inputs': list(inputs.keys()),
                }
            )
            
            # 5. Mise Ã  jour du step
            step.mark_success(artefact.hash)
            step.stdout = metadata.get('stdout', '')
            step.stderr = metadata.get('stderr', '')
            step.save(update_fields=['stdout', 'stderr'])
            
            # 6. Enregistrement dans output_artefacts du run
            run.output_artefacts[step.node_id] = artefact.hash
            run.save(update_fields=['output_artefacts'])
            
            logger.info(
                f"âœ… Step completed: {step.node_id} -> {artefact.hash[:8]}"
            )
        
        except SandboxExecutionError as e:
            error_msg = f"Sandbox execution failed: {e}"
            step.mark_failed(error_msg)
            logger.error(error_msg)
            raise
        
        except Exception as e:
            error_msg = f"Step execution error: {e}"
            step.mark_failed(error_msg)
            logger.error(error_msg, exc_info=True)
            raise
    
    def _resolve_inputs(
        self,
        run: PipelineRun,
        step: StepRun
    ) -> Dict[str, Any]:
        """
        RÃ©sout les inputs d'un step depuis les artefacts des Ã©tapes prÃ©cÃ©dentes.
        
        Workflow :
        1. RÃ©cupÃ¨re les edges entrants du node
        2. Pour chaque edge, rÃ©cupÃ¨re l'artefact produit par le node source
        3. DÃ©sÃ©rialise les artefacts
        4. Combine avec les inputs du manifest
        
        Args:
            run: PipelineRun
            step: StepRun
        
        Returns:
            Dictionnaire des inputs rÃ©solus
        """
        inputs = {}
        
        # 1. Inputs depuis le manifest (fournis par l'utilisateur)
        node_id = step.node_id
        if node_id in run.input_manifest:
            inputs.update(run.input_manifest[node_id])
        
        # 2. Inputs depuis les artefacts des Ã©tapes prÃ©cÃ©dentes
        graph = run.pipeline.graph
        edges = graph.get('edges', [])
        
        for edge in edges:
            if edge['to'] == node_id:
                # RÃ©cupÃ¨re l'artefact du node source
                source_node_id = edge['from']
                
                if source_node_id in run.output_artefacts:
                    artefact_hash = run.output_artefacts[source_node_id]
                    
                    # Chargement de l'artefact
                    try:
                        artefact_obj = artefact_service.load_artefact(
                            artefact_hash,
                            log_access=False
                        )
                        
                        # Mapping du port de sortie vers port d'entrÃ©e
                        in_port = edge.get('in_port', 'input')
                        inputs[in_port] = artefact_obj
                        
                    except Exception as e:
                        logger.error(
                            f"Failed to load artefact {artefact_hash}: {e}"
                        )
                        raise

        log = {
            "node_id": node_id,
            "input": inputs
        }
        
        return inputs
    
    def cancel_run(self, run_id: str) -> PipelineRun:
        """
        Annule une exÃ©cution en cours.
        
        Note: En mode synchrone, l'annulation est difficile (thread bloquÃ©).
        En mode asynchrone (Celery), on peut rÃ©voquer les tasks.
        
        Args:
            run_id: UUID du PipelineRun
        
        Returns:
            PipelineRun mis Ã  jour
        """
        run = PipelineRun.objects.get(id=run_id)
        
        if run.status not in ['PENDING', 'RUNNING']:
            raise ValueError(f"Cannot cancel run in status {run.status}")
        
        # Annulation des steps en cours
        run.step_runs.filter(status='PENDING').update(status='SKIPPED')
        run.step_runs.filter(status='RUNNING').update(status='FAILED')
        
        run.mark_cancelled()
        
        logger.info(f"ðŸš« Run cancelled: {run.id}")
        
        return run


# Instance globale
execution_service = ExecutionService()