# apps/artefacts/storage.py
"""
Gestionnaire de stockage des artefacts sur filesystem.

Architecture :
storage/artefacts/
‚îú‚îÄ‚îÄ by_hash/
‚îÇ   ‚îú‚îÄ‚îÄ e3/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ e3b0c442...b855.zst
‚îÇ   ‚îî‚îÄ‚îÄ a4/
‚îÇ       ‚îî‚îÄ‚îÄ a4d55a8d...4e5b.zst

Choix techniques :
- Compression zstandard (niveau 3 par d√©faut) : bon compromis vitesse/taux
- Hash SHA-256 sur contenu NON compress√© pour d√©duplication
- Organisation par pr√©fixe (2 premiers caract√®res) pour √©viter trop de fichiers/dossier
- √âcriture atomique via fichier temporaire
"""
import os
import hashlib
import cloudpickle
import zstandard as zstd
import logging
from pathlib import Path
from typing import Optional, Tuple, BinaryIO
from django.conf import settings

logger = logging.getLogger("notelib")


class ArtefactStorage:
    """
    G√®re le stockage et la r√©cup√©ration des artefacts sur filesystem.
    
    Responsabilit√©s :
    - S√©rialisation (cloudpickle) + compression (zstd)
    - Calcul du hash SHA-256
    - Stockage organis√© par pr√©fixe
    - Lecture avec streaming pour gros fichiers
    """
    
    # Configuration par d√©faut
    DEFAULT_COMPRESSION_LEVEL = 3  # zstd: 1-22 (3 = bon compromis)
    HASH_PREFIX_LENGTH = 2  # Nombre de caract√®res pour le sous-dossier
    
    def __init__(self, base_dir: Optional[str] = None):
        """
        Initialise le gestionnaire de stockage.
        
        Args:
            base_dir: R√©pertoire racine. Si None, utilise settings.ARTEFACT_STORAGE_DIR
        """
        if base_dir is None:
            base_dir = getattr(
                settings,
                'ARTEFACT_STORAGE_DIR',
                os.path.join(settings.BASE_DIR, 'storage', 'artefacts')
            )
        
        self.base_dir = Path(base_dir)
        self.hash_dir = self.base_dir / "by_hash"
        
        # Cr√©ation des r√©pertoires
        self.hash_dir.mkdir(parents=True, exist_ok=True)
        
        # Configuration compression
        self.compression_level = getattr(
            settings,
            'ARTEFACT_COMPRESSION_LEVEL',
            self.DEFAULT_COMPRESSION_LEVEL
        )
        
        logger.debug(f"ArtefactStorage initialized at {self.base_dir}")
    
    def _get_hash_path(self, hash_value: str) -> Path:
        """
        G√©n√®re le chemin de stockage bas√© sur le hash.
        
        Args:
            hash_value: Hash SHA-256 (64 caract√®res hex)
        
        Returns:
            Chemin complet : .../by_hash/e3/e3b0c442...zst
        """
        prefix = hash_value[:self.HASH_PREFIX_LENGTH]
        subdir = self.hash_dir / prefix
        subdir.mkdir(exist_ok=True)
        return subdir / f"{hash_value}.zst"
    
    def _get_relative_path(self, hash_value: str) -> str:
        """
        Retourne le chemin relatif pour stockage en BDD.
        
        Args:
            hash_value: Hash SHA-256
        
        Returns:
            Chemin relatif : "by_hash/e3/e3b0c442...zst"
        """
        prefix = hash_value[:self.HASH_PREFIX_LENGTH]
        return f"by_hash/{prefix}/{hash_value}.zst"
    
    def compute_hash(self, data: bytes) -> str:
        """
        Calcule le SHA-256 d'un contenu binaire.
        
        Args:
            data: Donn√©es binaires
        
        Returns:
            Hash SHA-256 en hexad√©cimal (64 caract√®res)
        """
        return hashlib.sha256(data).hexdigest()
    
    def serialize_and_compress(
        self,
        obj: object
    ) -> Tuple[bytes, bytes, str]:
        """
        S√©rialise un objet Python et le compresse.
        
        Workflow :
        1. S√©rialisation avec cloudpickle
        2. Calcul du hash sur le binaire non compress√©
        3. Compression avec zstd
        
        Args:
            obj: Objet Python √† s√©rialiser
        
        Returns:
            Tuple (donn√©es_compress√©es, donn√©es_brutes, hash)
        
        Raises:
            Exception: Si s√©rialisation ou compression √©choue
        """
        try:
            # S√©rialisation
            raw_data = cloudpickle.dumps(obj)
            
            # Hash du contenu non compress√© (pour d√©duplication)
            hash_value = self.compute_hash(raw_data)
            
            # Compression
            compressor = zstd.ZstdCompressor(level=self.compression_level)
            compressed_data = compressor.compress(raw_data)
            
            logger.debug(
                f"Serialized and compressed: {len(raw_data)} -> {len(compressed_data)} bytes "
                f"(ratio: {len(compressed_data)/len(raw_data):.2%})"
            )
            
            return compressed_data, raw_data, hash_value
            
        except Exception as e:
            logger.error(f"Failed to serialize/compress object: {e}")
            raise
    
    def decompress_and_deserialize(self, compressed_data: bytes) -> object:
        """
        D√©compresse et d√©s√©rialise un artefact.
        
        Args:
            compressed_data: Donn√©es compress√©es en zstd
        
        Returns:
            Objet Python d√©s√©rialis√©
        
        Raises:
            Exception: Si d√©compression ou d√©s√©rialisation √©choue
        """
        try:
            # D√©compression
            decompressor = zstd.ZstdDecompressor()
            raw_data = decompressor.decompress(compressed_data)
            
            # D√©s√©rialisation
            obj = cloudpickle.loads(raw_data)
            
            return obj
            
        except Exception as e:
            logger.error(f"Failed to decompress/deserialize: {e}")
            raise
    
    def exists(self, hash_value: str) -> bool:
        """
        V√©rifie si un artefact existe d√©j√†.
        
        Args:
            hash_value: Hash SHA-256
        
        Returns:
            True si le fichier existe
        """
        file_path = self._get_hash_path(hash_value)
        return file_path.exists()
    
    def save(
        self,
        obj: object,
        hash_override: Optional[str] = None
    ) -> Tuple[str, int, int, str]:
        """
        Sauvegarde un objet s√©rialis√© et compress√©.
        
        Args:
            obj: Objet Python √† persister
            hash_override: Hash pr√©-calcul√© (optionnel)
        
        Returns:
            Tuple (chemin_relatif, taille_compress√©e, taille_brute, hash)
        
        Raises:
            IOError: Si √©criture √©choue
            ValueError: Si hash ne correspond pas
        """
        # S√©rialisation + compression
        compressed_data, raw_data, computed_hash = self.serialize_and_compress(obj)
        
        # V√©rification du hash si fourni
        if hash_override and hash_override != computed_hash:
            raise ValueError(
                f"Hash mismatch: expected {hash_override}, got {computed_hash}"
            )
        
        hash_value = hash_override or computed_hash
        file_path = self._get_hash_path(hash_value)
        
        # V√©rification si d√©j√† existant (d√©duplication)
        if file_path.exists():
            logger.info(f"Artefact already exists (deduplicated): {hash_value[:8]}")
            return (
                self._get_relative_path(hash_value),
                len(compressed_data),
                len(raw_data),
                hash_value
            )
        
        try:
            # √âcriture atomique via fichier temporaire
            temp_path = file_path.with_suffix('.tmp')
            temp_path.write_bytes(compressed_data)
            temp_path.rename(file_path)
            
            relative_path = self._get_relative_path(hash_value)
            
            logger.info(
                f"üíæ Artefact saved: {relative_path} "
                f"({len(compressed_data)} bytes, ratio: {len(compressed_data)/len(raw_data):.2%})"
            )
            
            return relative_path, len(compressed_data), len(raw_data), hash_value
            
        except Exception as e:
            logger.error(f"‚ùå Failed to save artefact {hash_value}: {e}")
            # Nettoyage du fichier temporaire
            if temp_path.exists():
                temp_path.unlink()
            raise
    
    def load(self, hash_value: str) -> object:
        """
        Charge et d√©s√©rialise un artefact.
        
        Args:
            hash_value: Hash SHA-256
        
        Returns:
            Objet Python d√©s√©rialis√©
        
        Raises:
            FileNotFoundError: Si l'artefact n'existe pas
        """
        file_path = self._get_hash_path(hash_value)
        
        if not file_path.exists():
            raise FileNotFoundError(f"Artefact not found: {hash_value}")
        
        try:
            compressed_data = file_path.read_bytes()
            obj = self.decompress_and_deserialize(compressed_data)
            
            logger.debug(f"‚úÖ Artefact loaded: {hash_value[:8]}")
            
            return obj
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load artefact {hash_value}: {e}")
            raise
    
    def stream(self, hash_value: str, chunk_size: int = 8192) -> BinaryIO:
        """
        Ouvre un artefact en mode streaming (pour gros fichiers).
        
        Args:
            hash_value: Hash SHA-256
            chunk_size: Taille des chunks
        
        Returns:
            File object binaire
        
        Raises:
            FileNotFoundError: Si l'artefact n'existe pas
        """
        file_path = self._get_hash_path(hash_value)
        
        if not file_path.exists():
            raise FileNotFoundError(f"Artefact not found: {hash_value}")
        
        return open(file_path, 'rb')
    
    def delete(self, hash_value: str) -> bool:
        """
        Supprime un artefact du filesystem.
        
        Args:
            hash_value: Hash SHA-256
        
        Returns:
            True si supprim√©, False si n'existait pas
        """
        file_path = self._get_hash_path(hash_value)
        
        if file_path.exists():
            file_path.unlink()
            logger.info(f"üóëÔ∏è  Artefact deleted: {hash_value[:8]}")
            return True
        
        return False
    
    def get_size(self, hash_value: str) -> Optional[int]:
        """
        Retourne la taille d'un artefact en octets.
        
        Args:
            hash_value: Hash SHA-256
        
        Returns:
            Taille en octets ou None si inexistant
        """
        file_path = self._get_hash_path(hash_value)
        
        if file_path.exists():
            return file_path.stat().st_size
        
        return None
    
    def cleanup_orphans(self, existing_hashes: set) -> int:
        """
        Nettoie les fichiers orphelins (sans entr√©e en BDD).
        
        Args:
            existing_hashes: Set des hash pr√©sents en BDD
        
        Returns:
            Nombre de fichiers supprim√©s
        """
        deleted_count = 0
        
        for subdir in self.hash_dir.iterdir():
            if not subdir.is_dir():
                continue
            
            for file_path in subdir.glob("*.zst"):
                hash_value = file_path.stem
                
                if hash_value not in existing_hashes:
                    file_path.unlink()
                    deleted_count += 1
                    logger.info(f"üóëÔ∏è  Orphan artefact deleted: {hash_value[:8]}")
        
        logger.info(f"üßπ Cleanup completed: {deleted_count} orphan files deleted")
        
        return deleted_count


# Instance globale
artefact_storage = ArtefactStorage()